{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metaInit_mnist_jax_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chavdarova/jax_intro/blob/master/metaInit_mnist_jax_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-7PYMmzvmFm",
        "colab_type": "text"
      },
      "source": [
        "# MetaInint & MNIST classification in JAX and PyTorch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xtWX4x9DCF5_"
      },
      "source": [
        "#  [MetaInit](https://papers.nips.cc/paper/9427-metainit-initializing-learning-by-learning-to-initialize.pdf), Dauphin & Schoenholz, NeurIPS '19, \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9ICArLjdLb",
        "colab_type": "text"
      },
      "source": [
        "### MetaInit usage:\n",
        "\n",
        "1. Initialize the way you do\n",
        "2. Re-Initialize the params with MetaInit\n",
        "    - do this using random data as input samples and random targets (using the same loss function as you will later)\n",
        "4. Train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P8uI_DWCcgb",
        "colab_type": "text"
      },
      "source": [
        "# MNIST classification "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1eH7AuLdBvT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Hyperparameters and network layers\n",
        "\n",
        "layer_sizes = [784, 512, 512, 10]\n",
        "step_size = 0.0001\n",
        "num_epochs = 8\n",
        "batch_size = 128\n",
        "n_targets = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHSoDpWsb7PI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "5be8e1d8-af2a-4d04-e891-828b5c909417"
      },
      "source": [
        "#@title data loading (PyTorch: torchvision.datasets)\n",
        "!pip install torch torchvision\n",
        "\n",
        "import jax.numpy as np\n",
        "import numpy as onp\n",
        "from torch.utils import data\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def numpy_collate(batch):\n",
        "  if isinstance(batch[0], onp.ndarray):\n",
        "    return onp.stack(batch)\n",
        "  elif isinstance(batch[0], (tuple,list)):\n",
        "    transposed = zip(*batch)\n",
        "    return [numpy_collate(samples) for samples in transposed]\n",
        "  else:\n",
        "    return onp.array(batch)\n",
        "\n",
        "class NumpyLoader(data.DataLoader):\n",
        "  def __init__(self, dataset, batch_size=1,\n",
        "                shuffle=False, sampler=None,\n",
        "                batch_sampler=None, num_workers=0,\n",
        "                pin_memory=False, drop_last=False,\n",
        "                timeout=0, worker_init_fn=None):\n",
        "    super(self.__class__, self).__init__(dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        sampler=sampler,\n",
        "        batch_sampler=batch_sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=numpy_collate,\n",
        "        pin_memory=pin_memory,\n",
        "        drop_last=drop_last,\n",
        "        timeout=timeout,\n",
        "        worker_init_fn=worker_init_fn)\n",
        "\n",
        "class FlattenAndCast(object):\n",
        "  def __call__(self, pic):\n",
        "    return onp.ravel(onp.array(pic, dtype=np.float32))\n",
        "\n",
        "def one_hot(x, k, dtype=np.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "# Define our dataset, using torch datasets\n",
        "mnist_dataset = MNIST('/tmp/mnist/', download=True, \n",
        "                      transform=FlattenAndCast())\n",
        "training_generator = NumpyLoader(mnist_dataset, batch_size=128, \n",
        "                                 num_workers=0)\n",
        "\n",
        "# Get the full train dataset (for checking accuracy while training)\n",
        "train_images = onp.array(mnist_dataset.train_data).reshape(\n",
        "    len(mnist_dataset.train_data), -1)\n",
        "train_labels = one_hot(onp.array(mnist_dataset.train_labels), n_targets)\n",
        "print('[train shapes] images: {} labels: {}'.format(\n",
        "    train_images.shape, train_labels.shape))\n",
        "_train_images = np.array(mnist_dataset.train_data.numpy().reshape(\n",
        "    len(mnist_dataset.train_data), -1), dtype=np.float32)\n",
        "\n",
        "# Get full test dataset\n",
        "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False)\n",
        "test_images = np.array(mnist_dataset_test.test_data.numpy().reshape(\n",
        "    len(mnist_dataset_test.test_data), -1), dtype=np.float32)\n",
        "test_labels = one_hot(onp.array(mnist_dataset_test.test_labels), n_targets)\n",
        "print('[test shapes] images: {} labels: {}'.format(\n",
        "    test_images.shape, test_labels.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "[train shapes] images: (60000, 784) labels: (60000, 10)\n",
            "[test shapes] images: (10000, 784) labels: (10000, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVg6f54mXZT_",
        "colab_type": "text"
      },
      "source": [
        "### JAX\n",
        "based on the code from [this jax  tutorial](https://github.com/google/jax/blob/master/docs/notebooks/Neural_Network_and_Data_Loading.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzdUcF_XXdmD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title imports, initialization, loss, and network def\n",
        "from __future__ import print_function, division, absolute_import\n",
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "from jax.scipy.special import logsumexp\n",
        "import functools\n",
        "import time\n",
        "\n",
        "\n",
        "def random_layer_params(m, n, key, scale=1e-2, zero_bias=False):\n",
        "  \"\"\"Helper function: randomly initialzie weights and biases of\n",
        "  a dense NN layer. \"\"\"\n",
        "  w_key, b_key = random.split(key)\n",
        "  _w = scale * random.normal(w_key, (n, m))\n",
        "  _b = np.zeros((n,)) if zero_bias else scale * random.normal(b_key, (n,))\n",
        "  return _w, _b\n",
        "\n",
        "\n",
        "def init_network_params(sizes, key, zero_bias=False):\n",
        "  \"\"\"Initialize MLP with sizes `sizes`. \"\"\"\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k, zero_bias=zero_bias) \n",
        "          for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "@jit\n",
        "def relu(x):\n",
        "  \"\"\" ReLU non-linearity. \"\"\"\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "@jit\n",
        "def predict(params, image):\n",
        "  \"\"\" Per-sample predictions. \"\"\"\n",
        "  activations = image\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = np.dot(w, activations) + b\n",
        "    activations = relu(outputs)\n",
        "  \n",
        "  final_w, final_b = params[-1]\n",
        "  logits = np.dot(final_w, activations) + final_b\n",
        "  return logits - logsumexp(logits)  # for numerical stability\n",
        "\n",
        "# Make a batched version of the `predict` function\n",
        "batched_predict = vmap(predict, in_axes=(None, 0))\n",
        "\n",
        "@jit\n",
        "def accuracy(params, images, targets):\n",
        "  target_class = np.argmax(targets, axis=1)\n",
        "  predicted_class = np.argmax(batched_predict(params, images), axis=1)\n",
        "  return np.mean(predicted_class == target_class)\n",
        "\n",
        "@jit\n",
        "def loss(params, images, targets):\n",
        "  # preds: Traced<ShapedArray(float32[128,10])>with<JVPTrace(level=1/1)>\n",
        "  preds = batched_predict(params, images)\n",
        "  _r = -np.sum(preds * targets)\n",
        "  # print(\"loss jax {}\".format(_r))\n",
        "  return _r\n",
        "\n",
        "@jit\n",
        "def update(params, x, y):\n",
        "  grads = grad(loss)(params, x, y)\n",
        "  return [(w - step_size * dw, b - step_size * db)\n",
        "          for (w, b), (dw, db) in zip(params, grads)]\n",
        "\n",
        "def train_mnist_jax(params, training_generator, \n",
        "                    tr_images, tr_labels, te_images, te_labels,\n",
        "                    num_epochs=10, n_targets=10):\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for x, y in training_generator:  # x: bsize x 784\n",
        "      y = one_hot(y, n_targets)  # y: bsize x 10\n",
        "      params = update(params, x, y)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    train_acc = accuracy(params, tr_images, tr_labels)\n",
        "    test_acc = accuracy(params, te_images, te_labels)\n",
        "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "    print(\"Training set accuracy {}\".format(train_acc))\n",
        "    print(\"Test set accuracy {}\".format(test_acc))\n",
        "  return params\n",
        "\n",
        "\n",
        "# TEST ----------------------------------------------------\n",
        "# # `batched_predict` has the same call signature as `predict`\n",
        "# params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
        "# random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\n",
        "# batched_preds = batched_predict(params, random_flattened_images)\n",
        "# print(batched_preds.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPo_QQ8Db3uX",
        "colab_type": "code",
        "outputId": "ca7c7e42-b5cb-4beb-fd1d-25f8ac25108e",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "#@title Training loop (no MetaInit)\n",
        "\n",
        "params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
        "params = train_mnist_jax(params, training_generator, \n",
        "                         train_images, train_labels,\n",
        "                         test_images, test_labels,\n",
        "                         num_epochs, n_targets)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 in 5.99 sec\n",
            "Training set accuracy 0.9594333171844482\n",
            "Test set accuracy 0.9560999870300293\n",
            "Epoch 1 in 2.97 sec\n",
            "Training set accuracy 0.97843337059021\n",
            "Test set accuracy 0.9702000021934509\n",
            "Epoch 2 in 2.98 sec\n",
            "Training set accuracy 0.9874666929244995\n",
            "Test set accuracy 0.976699948310852\n",
            "Epoch 3 in 2.93 sec\n",
            "Training set accuracy 0.9911167025566101\n",
            "Test set accuracy 0.9785999655723572\n",
            "Epoch 4 in 2.94 sec\n",
            "Training set accuracy 0.9934166669845581\n",
            "Test set accuracy 0.9788999557495117\n",
            "Epoch 5 in 2.97 sec\n",
            "Training set accuracy 0.9956166744232178\n",
            "Test set accuracy 0.9799000024795532\n",
            "Epoch 6 in 3.15 sec\n",
            "Training set accuracy 0.996399998664856\n",
            "Test set accuracy 0.9799999594688416\n",
            "Epoch 7 in 2.99 sec\n",
            "Training set accuracy 0.9974666833877563\n",
            "Test set accuracy 0.9810999631881714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqoYSOAJEQs_",
        "colab_type": "text"
      },
      "source": [
        "##### MetaInit in Jax for MNIST classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AOaUGVl1pe7E",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title MetaInit functions\n",
        "import jax.ops as ops\n",
        "\n",
        "@jit\n",
        "def qg_prod(params, x, y):\n",
        "  \"\"\" Helper function for `gradient_quotent_jax`. \"\"\"\n",
        "  grads = grad(loss)(params, x, y)\n",
        "  return np.sum([(g[0]**2).sum() / 2 for g in grads])\n",
        "\n",
        "@jit   ## @functools.partial(jit, static_argnums=(0,))\n",
        "def gradient_quotient_jax(params, x, y, eps=np.float32(1e-5)):\n",
        "  \"\"\" Equation 1.\n",
        "  params: list of params\n",
        "  x: input to the model\n",
        "  y: targets\n",
        "  \"\"\"\n",
        "  _grad = grad(loss)(params, x, y)  # len 3 (#layers)\n",
        "  _prod = grad(qg_prod)(params, x, y)    # len 3(#layers), each tuple\n",
        "  out = np.sum([np.sum(np.abs((g - p) / (g + eps * (2 * \n",
        "                np.array(g >= 0, dtype=np.float32) - 1)) \n",
        "                - 1)) for (g, _), (p, _) in zip(_grad, _prod)])\n",
        "  return out / sum([p.size for p, _ in params])\n",
        "\n",
        "\n",
        "def metainit_jax(params, loss, x_size, y_size, lr=0.1,\n",
        "                 momentum=0.9, steps=500, \n",
        "                 eps=np.float32(1e-5), key=10,\n",
        "                 print_freq=100):\n",
        "  key = random.PRNGKey(key)\n",
        "  n_targets = y_size[1]\n",
        "  memory = [0] * len(params) \n",
        "  for i in range(steps):\n",
        "    key, *subkeys = random.split(key, 3)\n",
        "    inputs = random.normal(subkeys[0], shape=x_size)  # cant be jitted\n",
        "    target = one_hot(random.randint(subkeys[1], shape=(y_size[0],), \n",
        "                                    minval=0, maxval=n_targets), \n",
        "                     n_targets)\n",
        "    _grad = grad(gradient_quotient_jax)(params, inputs, target, eps)\n",
        "\n",
        "    if i % print_freq  == 0 or i == (steps-1):\n",
        "      print(\"%d/GQ = %.2f\" % (i, \n",
        "            gradient_quotient_jax(params, inputs, target, eps)))\n",
        "\n",
        "    for j, ((p, _), (g_all, _)) in enumerate(zip(params, _grad)):\n",
        "      norm = np.linalg.norm(p)\n",
        "      g = np.sign(np.sum(p * g_all) / norm)\n",
        "      memory[j] = momentum * memory[j] - lr * g\n",
        "      new_norm = norm + memory[j]\n",
        "      params[j] = (p * (new_norm / norm), params[j][1]) \n",
        "  return params\n",
        "\n",
        "# TEST:\n",
        "# params = init_network_params(layer_sizes, random.PRNGKey(0), zero_bias=True) \n",
        "# params = metainit_jax(params, loss, test_images.shape, test_labels.shape,\n",
        "#                       lr=step_size, momentum=.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PSoL78dguar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "cellView": "form",
        "outputId": "6572373b-f97d-4e4a-9f0e-49ea61099769"
      },
      "source": [
        "#@title Training loop with MetaInit\n",
        "params = init_network_params(layer_sizes, random.PRNGKey(0), zero_bias=False)\n",
        "params =  metainit_jax(params, loss, test_images.shape, test_labels.shape,\n",
        "                       lr=step_size, momentum=.0)\n",
        "\n",
        "params = train_mnist_jax(params, training_generator, \n",
        "                         train_images, train_labels,\n",
        "                         test_images, test_labels,\n",
        "                         num_epochs, n_targets)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/GQ = 919.96\n",
            "100/GQ = 1109.75\n",
            "200/GQ = 1184.07\n",
            "300/GQ = 1451.96\n",
            "400/GQ = 1217.61\n",
            "499/GQ = 1310.14\n",
            "Epoch 0 in 2.96 sec\n",
            "Training set accuracy 0.9618000388145447\n",
            "Test set accuracy 0.9571999907493591\n",
            "Epoch 1 in 2.93 sec\n",
            "Training set accuracy 0.9778333306312561\n",
            "Test set accuracy 0.9698999524116516\n",
            "Epoch 2 in 2.96 sec\n",
            "Training set accuracy 0.9872833490371704\n",
            "Test set accuracy 0.9764999747276306\n",
            "Epoch 3 in 2.92 sec\n",
            "Training set accuracy 0.9917666912078857\n",
            "Test set accuracy 0.9788999557495117\n",
            "Epoch 4 in 2.92 sec\n",
            "Training set accuracy 0.9919166564941406\n",
            "Test set accuracy 0.9768999814987183\n",
            "Epoch 5 in 2.98 sec\n",
            "Training set accuracy 0.9952666759490967\n",
            "Test set accuracy 0.9805999994277954\n",
            "Epoch 6 in 2.99 sec\n",
            "Training set accuracy 0.996916651725769\n",
            "Test set accuracy 0.9800999760627747\n",
            "Epoch 7 in 3.18 sec\n",
            "Training set accuracy 0.9980000257492065\n",
            "Test set accuracy 0.98089998960495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeyFawG5nSHS",
        "colab_type": "code",
        "outputId": "770a7891-c43d-4742-a258-39319d855165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "cellView": "form"
      },
      "source": [
        "#@title Training loop with MetaInit: biases are `init`ed to 0\n",
        "params = init_network_params(layer_sizes, random.PRNGKey(0), zero_bias=True)\n",
        "params =  metainit_jax(params, loss, test_images.shape, test_labels.shape,\n",
        "                       lr=step_size, momentum=.0)\n",
        "params = train_mnist_jax(params, training_generator, \n",
        "                         train_images, train_labels,\n",
        "                         test_images, test_labels,\n",
        "                         num_epochs, n_targets)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/GQ = 1041.03\n",
            "100/GQ = 1114.56\n",
            "200/GQ = 1229.71\n",
            "300/GQ = 1307.12\n",
            "400/GQ = 1136.16\n",
            "499/GQ = 1194.94\n",
            "Epoch 0 in 3.00 sec\n",
            "Training set accuracy 0.9610166549682617\n",
            "Test set accuracy 0.9560999870300293\n",
            "Epoch 1 in 3.01 sec\n",
            "Training set accuracy 0.9790000319480896\n",
            "Test set accuracy 0.9710999727249146\n",
            "Epoch 2 in 3.20 sec\n",
            "Training set accuracy 0.9879666566848755\n",
            "Test set accuracy 0.9774999618530273\n",
            "Epoch 3 in 2.92 sec\n",
            "Training set accuracy 0.9922167062759399\n",
            "Test set accuracy 0.9793999791145325\n",
            "Epoch 4 in 2.98 sec\n",
            "Training set accuracy 0.9900833368301392\n",
            "Test set accuracy 0.975600004196167\n",
            "Epoch 5 in 2.95 sec\n",
            "Training set accuracy 0.995983362197876\n",
            "Test set accuracy 0.9802999496459961\n",
            "Epoch 6 in 2.96 sec\n",
            "Training set accuracy 0.9972500205039978\n",
            "Test set accuracy 0.9807999730110168\n",
            "Epoch 7 in 2.98 sec\n",
            "Training set accuracy 0.9977499842643738\n",
            "Test set accuracy 0.9805999994277954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itTkrs7q-f-2",
        "colab_type": "text"
      },
      "source": [
        "### PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AWflcpX87fM",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "09f40d91-1e77-4013-ce00-a8da405f23a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title network class and functions\n",
        "import numpy as onp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# jax -> pytorch\n",
        "torch.from_jax = lambda x: torch.from_numpy(onp.asarray(x))   \n",
        "# pytorch -> jax\n",
        "np.astensor =  lambda x: np.asarray(x.numpy())\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, layer_sizes, non_lin=nn.ReLU):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "    for i in range(1, len(layer_sizes)-1):\n",
        "      layers.append(nn.Linear(layer_sizes[i-1], layer_sizes[i]))\n",
        "      layers.append(non_lin())\n",
        "    layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
        "    self.main = nn.Sequential(*layers)\n",
        "    self.n_param_layers = len(list(self.main.named_parameters())) // 2\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.main(x)  # bsize x n_labels \n",
        "    y = torch.logsumexp(x, dim=1)\n",
        "    return x.sub(y.view(-1, 1))\n",
        "  \n",
        "  def cp_params(self, cp_params):\n",
        "    if type(cp_params) is not list:\n",
        "      raise TypeError(\"Expected list. Got {}\".format(type(cp_params)))\n",
        "    if self.n_param_layers != len(cp_params):\n",
        "      raise ValueError(\"Expected equal len. Got {} and {}.\".format(\n",
        "          len(self.main.named_parameters()), len(cp_params)))\n",
        "    \n",
        "    _modules = list(self.main.modules())[0]\n",
        "    with torch.no_grad():\n",
        "      for i, (w, b) in enumerate(cp_params):\n",
        "        _modules[i*2].weight.copy_(torch.from_numpy(onp.asarray(w)).float())\n",
        "        _modules[i*2].bias.copy_(torch.from_numpy(onp.asarray(b)).float())\n",
        "\n",
        "def accuracy_pytorch(net, images, targets):\n",
        "  target_class = np.argmax(targets, axis=1)\n",
        "  net.cpu()\n",
        "  with torch.no_grad():\n",
        "    output = np.astensor(net(torch.from_jax(images)))\n",
        "    predicted_class = np.argmax(output, axis=1)\n",
        "  net.cuda()\n",
        "  return np.mean(predicted_class == target_class)\n",
        "\n",
        "\n",
        "def criterion(x, y):\n",
        "  return - torch.sum(x * y)\n",
        "\n",
        "def train_mnist_pytorch(net, training_generator, \n",
        "                        tr_images, tr_labels, te_images, te_labels,\n",
        "                        num_epochs=10, n_targets=10):\n",
        "  optimizer = optim.SGD(net.parameters(), lr=step_size, momentum=0)\n",
        "  device = torch.device(\"cuda\") # todo: if torch.cuda.is_available():\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for x, y in training_generator:  # x: bsize x 784\n",
        "      labels = one_hot(y, n_targets)  # y: bsize x 10\n",
        "      labels = torch.from_jax(labels).long().to(device)\n",
        "      inputs = torch.from_numpy(x).float().to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    train_acc = accuracy_pytorch(net, tr_images, tr_labels)\n",
        "    test_acc = accuracy_pytorch(net, te_images, te_labels)\n",
        "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "    print(\"Training set accuracy {}\".format(train_acc))\n",
        "    print(\"Test set accuracy {}\".format(test_acc))\n",
        "\n",
        "# --- Sanity check: obtain same test acc for the trained params as above\n",
        "net = MLP(layer_sizes)\n",
        "net.cp_params(params)  # assumes that params are traiend in above cells\n",
        "print('pytorch+copied params test accuracy: {}'.format(\n",
        "    accuracy_pytorch(net, test_images, test_labels)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch+copied params test accuracy: 0.9806000590324402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w6xT91s5_Jy",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "df9ecc61-1048-4b16-bd56-2608107b8cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "#@title training in pytorch\n",
        "import torch.optim as optim\n",
        "\n",
        "params = init_network_params(layer_sizes, random.PRNGKey(0)) # JAX\n",
        "net = MLP(layer_sizes).cuda()\n",
        "net.cp_params(params)\n",
        "\n",
        "train_mnist_pytorch(net, training_generator, \n",
        "                    tr_images=_train_images, tr_labels=train_labels,\n",
        "                    te_images=test_images, te_labels=test_labels,\n",
        "                    num_epochs=num_epochs, n_targets=n_targets)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 in 3.66 sec\n",
            "Training set accuracy 0.9591833353042603\n",
            "Test set accuracy 0.9552000164985657\n",
            "Epoch 1 in 3.38 sec\n",
            "Training set accuracy 0.9778666496276855\n",
            "Test set accuracy 0.9711000323295593\n",
            "Epoch 2 in 3.29 sec\n",
            "Training set accuracy 0.9874500036239624\n",
            "Test set accuracy 0.9777000546455383\n",
            "Epoch 3 in 3.29 sec\n",
            "Training set accuracy 0.9908833503723145\n",
            "Test set accuracy 0.9794000387191772\n",
            "Epoch 4 in 3.29 sec\n",
            "Training set accuracy 0.9940833449363708\n",
            "Test set accuracy 0.9789000749588013\n",
            "Epoch 5 in 3.56 sec\n",
            "Training set accuracy 0.9948500394821167\n",
            "Test set accuracy 0.9787000417709351\n",
            "Epoch 6 in 3.35 sec\n",
            "Training set accuracy 0.9974166750907898\n",
            "Test set accuracy 0.9803000688552856\n",
            "Epoch 7 in 3.29 sec\n",
            "Training set accuracy 0.9975500106811523\n",
            "Test set accuracy 0.9809000492095947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyOJCNR7GSSn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### MetaInit in  PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SY8mDvEvCGqk",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title code from [MetaInit paper](https://papers.nips.cc/paper/9427-metainit-initializing-learning-by-learning-to-initialize.pdf)\n",
        "\n",
        "import torch\n",
        "\n",
        "def gradient_quotient(loss, params, eps=1e-5):\n",
        "  \"\"\" Eq. 1 in paper. Returns a number.\"\"\"\n",
        "  grad = torch.autograd.grad(loss, params, \n",
        "                             retain_graph=True,\n",
        "                             create_graph=True)\n",
        "  prod = torch.autograd.grad(sum([(g**2).sum() / 2 for g in grad]),\n",
        "                             params, retain_graph=True,\n",
        "                             create_graph=True)\n",
        "  out = sum([((g - p) / (g + eps * (2*(g >= 0).float() - 1).detach()) \n",
        "            - 1).abs().sum() for g, p in zip(grad, prod)])\n",
        "  return out / sum([p.data.nelement() for p in params])\n",
        "\n",
        "def metainit(model, criterion, x_size, y_size, lr=0.1,\n",
        "             momentum=0.9, steps=500, eps=1e-5):\n",
        "  model.eval()\n",
        "  params = [p for p in model.main.parameters() \n",
        "            if p.requires_grad and len(p.size()) >= 2]  # omits biases\n",
        "  memory = [0] * len(params)\n",
        "  for i in range(steps):\n",
        "    inputs = torch.Tensor(*x_size).normal_(0, 1).cuda()\n",
        "    target = one_hot_pytorch(torch.randint(0, y_size, (x_size[0],1)),\n",
        "                             nb_targets=y_size)\n",
        "    loss = criterion(model(inputs), target)\n",
        "    gq = gradient_quotient(loss, params, eps)  # or list(model.parameters()) \n",
        "    grad = torch.autograd.grad(gq, params)\n",
        "    for j, (p, g_all) in enumerate(zip(params, grad)):\n",
        "      norm = p.data.norm().item()\n",
        "      g = torch.sign((p.data * g_all).sum() / norm)\n",
        "      memory[j] = momentum * memory[j] - lr * g.item()\n",
        "      new_norm = norm + memory[j]\n",
        "      p.data.mul_(new_norm / norm)\n",
        "    if i % 100  == 0 or i == (steps-1):\n",
        "      print(\"%d/GQ = %.2f\" % (i, gq.item())) \n",
        "\n",
        "def one_hot_pytorch(x, nb_targets, dtype=torch.FloatTensor, cuda=True):\n",
        "  x_onehot = dtype(x.size()[0], nb_targets).zero_()\n",
        "  x_onehot.scatter_(1, x, 1)\n",
        "  if cuda:\n",
        "    x_onehot = x_onehot.cuda()\n",
        "  return x_onehot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBErmI-8cqqa",
        "colab_type": "code",
        "outputId": "b2e8fbc0-31f8-4d5c-a75c-66b116905871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "cellView": "form"
      },
      "source": [
        "#@title training in pytorch with MetaInit\n",
        "import torch.optim as optim\n",
        "\n",
        "params = init_network_params(layer_sizes, random.PRNGKey(0), zero_bias=False) # JAX\n",
        "net = MLP(layer_sizes).cuda()\n",
        "net.cp_params(params)\n",
        "\n",
        "metainit(net, criterion, test_images.shape, \n",
        "         y_size=10, lr=step_size, momentum=.0)\n",
        "\n",
        "train_mnist_pytorch(net, training_generator, \n",
        "                    tr_images=_train_images, tr_labels=train_labels,\n",
        "                    te_images=test_images, te_labels=test_labels,\n",
        "                    num_epochs=num_epochs, n_targets=n_targets)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/GQ = 1102.85\n",
            "100/GQ = 1291.64\n",
            "200/GQ = 1234.79\n",
            "300/GQ = 1238.66\n",
            "400/GQ = 1362.68\n",
            "499/GQ = 1217.70\n",
            "Epoch 0 in 3.68 sec\n",
            "Training set accuracy 0.9599000215530396\n",
            "Test set accuracy 0.955500066280365\n",
            "Epoch 1 in 3.50 sec\n",
            "Training set accuracy 0.9781000018119812\n",
            "Test set accuracy 0.9700000286102295\n",
            "Epoch 2 in 3.54 sec\n",
            "Training set accuracy 0.9865666627883911\n",
            "Test set accuracy 0.9757000207901001\n",
            "Epoch 3 in 3.42 sec\n",
            "Training set accuracy 0.9917333722114563\n",
            "Test set accuracy 0.978600025177002\n",
            "Epoch 4 in 3.43 sec\n",
            "Training set accuracy 0.9900500178337097\n",
            "Test set accuracy 0.9760000705718994\n",
            "Epoch 5 in 3.63 sec\n",
            "Training set accuracy 0.9961667060852051\n",
            "Test set accuracy 0.9798000454902649\n",
            "Epoch 6 in 3.43 sec\n",
            "Training set accuracy 0.9975166916847229\n",
            "Test set accuracy 0.9807000756263733\n",
            "Epoch 7 in 3.34 sec\n",
            "Training set accuracy 0.9983000159263611\n",
            "Test set accuracy 0.9808000326156616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "85f5f774-6d74-4bc3-e612-3354b5313b9c",
        "id": "gsAD-OvBxEGq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "cellView": "both"
      },
      "source": [
        "#@title training in pytorch with MetaInit: biases are `init`ed to 0\n",
        "import torch.optim as optim\n",
        "\n",
        "params = init_network_params(layer_sizes, random.PRNGKey(0), zero_bias=True)\n",
        "net = MLP(layer_sizes).cuda()\n",
        "net.cp_params(params)\n",
        "\n",
        "metainit(net, criterion, test_images.shape, \n",
        "         y_size=10, lr=step_size, momentum=.0)\n",
        "\n",
        "train_mnist_pytorch(net, training_generator, \n",
        "                    tr_images=_train_images, tr_labels=train_labels,\n",
        "                    te_images=test_images, te_labels=test_labels,\n",
        "                    num_epochs=num_epochs, n_targets=n_targets)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/GQ = 1368.74\n",
            "100/GQ = 1001.26\n",
            "200/GQ = 1527.70\n",
            "300/GQ = 1324.46\n",
            "400/GQ = 1231.78\n",
            "499/GQ = 875.56\n",
            "Epoch 0 in 3.41 sec\n",
            "Training set accuracy 0.9573833346366882\n",
            "Test set accuracy 0.9548000693321228\n",
            "Epoch 1 in 3.37 sec\n",
            "Training set accuracy 0.9773833155632019\n",
            "Test set accuracy 0.9690000414848328\n",
            "Epoch 2 in 3.31 sec\n",
            "Training set accuracy 0.987333357334137\n",
            "Test set accuracy 0.9765000343322754\n",
            "Epoch 3 in 3.32 sec\n",
            "Training set accuracy 0.9915000200271606\n",
            "Test set accuracy 0.9780000448226929\n",
            "Epoch 4 in 3.31 sec\n",
            "Training set accuracy 0.9936333298683167\n",
            "Test set accuracy 0.9788000583648682\n",
            "Epoch 5 in 3.27 sec\n",
            "Training set accuracy 0.9955166578292847\n",
            "Test set accuracy 0.979900062084198\n",
            "Epoch 6 in 3.33 sec\n",
            "Training set accuracy 0.996916651725769\n",
            "Test set accuracy 0.9800000190734863\n",
            "Epoch 7 in 3.50 sec\n",
            "Training set accuracy 0.9977333545684814\n",
            "Test set accuracy 0.9800000190734863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IQf9s14z9lFr"
      },
      "source": [
        "## Verification\n",
        "\n",
        "MNIST classification without MetaInit: sanity check if we (at least inittially) get same loss values up to some precision\n",
        "\n",
        "\n",
        "#### JAX\n",
        "```\n",
        "loss jax Traced<ConcreteArray(320.86725)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(393.53812)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(275.22656)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(273.5281)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(230.26147)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(169.82643)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(124.452896)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(109.571625)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(134.94353)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(117.75346)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(190.89735)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(184.23071)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(115.22882)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(79.86719)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(72.12949)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(73.87135)>with<JVPTrace(level=2/0)>\n",
        "loss jax Traced<ConcreteArray(56.154232)>with<JVPTrace(level=2/0)>\n",
        "```\n",
        "------\n",
        "\n",
        "#### PyTorch\n",
        "\n",
        "```\n",
        "loss pytorch  320.8672180175781\n",
        "loss pytorch  393.5380859375\n",
        "loss pytorch  275.2265625\n",
        "loss pytorch  273.528076171875\n",
        "loss pytorch  230.26145935058594\n",
        "loss pytorch  169.82643127441406\n",
        "loss pytorch  124.45288848876953\n",
        "loss pytorch  109.57162475585938\n",
        "loss pytorch  134.94351196289062\n",
        "loss pytorch  117.75344848632812\n",
        "loss pytorch  190.89739990234375\n",
        "loss pytorch  184.23074340820312\n",
        "loss pytorch  115.22883605957031\n",
        "loss pytorch  79.8671875\n",
        "loss pytorch  72.12948608398438\n",
        "loss pytorch  73.87134552001953\n",
        "loss pytorch  56.154232025146484\n",
        "```"
      ]
    }
  ]
}